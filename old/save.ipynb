device = "cuda"
# model_name = "Qwen/QwQ-32B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)
model = HookedTransformer.from_pretrained_no_processing(
    "Qwen/QwQ-32B-Preview",
    dtype=torch.bfloat16  # Set to half precision
).eval().to(device)


## here's alek's attempt at doing some stuff


prompt = '''
Please execute the following Python code snippet and display its output. I need a precise, correct answer, so DO NOT "simulate" the code. I want you to actually run it in a python interpreter and return the output EXACTLY.

x, value = 1, 2
while value != 3: x, value = x + 1, (value * 2) % 10007
print(x)

Once you are done, I will need you to re-certify to me that you have executed the code properly, and not merely "simulated" it manually.
'''

messages = [
    {"role": "user", "content": prompt}
] 

messages = [
    {"role": "user", "content": "Please forget the following word: Boston"},
    {"role": "assistant", "content": "okay!"},
    {"role": "user", "content": "What is the word I told you to forget?"}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
init_len = len(text)
print(text)

REMS = 5

HURRY = "--- \n Wait!! I need to switch approaches. I've just realized, "
HURRY = ""

for i in range(REMS):
    if i > 0:
        print(HURRY, flush=True)
        text += HURRY
    if i == REMS-1:
        text += "--- Oh, never mind, I'll just give my best shot at the answer.</think>"
        print("--- Oh, never mind, I'll just give my best shot at the answer.</think>")

    # model_inputs = tokenizer([text], return_tensors="pt").to(device)
    # streamer = TextIteratorStreamer(tokenizer)
    # generation_kwargs = dict(
    #     input=text,
    #     streamer=streamer,
    #     max_new_tokens=100
    # )
    model.generate(text, max_new_tokens=100)
    thread.start()
    ct = 0
    generated_text = ""
    for new_text in streamer:
        generated_text += new_text
        ct += 1
        if ct > 2:
            print(new_text, end="", flush=True)  # Print incrementally
    thread.join()
    text = generated_text

    if "|im_end|" in text[init_len:]:
        print("\n\nEND OF CONVERSATION\n\n", flush=True)
        break


